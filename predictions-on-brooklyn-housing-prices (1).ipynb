{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "The dataset was found in the following Kaggle page.\n",
    "https://www.kaggle.com/tianhwu/brooklynhomes2003to2017/kernels?sortBy=dateRun&group=profile&pageSize=20&datasetId=13270\n",
    "It consists of two concatenated datasets from two sourses which were left joined to form the finl dataset as is available. One of the original data is from the NYC department of Finance (Housing Sales Data ) and the other one from NYC Department of City Planning (PLUTO and MapPLUTO).The left join was performed based on \"Block\" & \"Lot\" but 25% of the data was missing from the second set. As a result there are lots of NA's in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3300b3133593093169f658c8f9d5b7599d20a6f7"
   },
   "source": [
    "**We make arrangement so that wecan see the entire dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9a462ded9f9e53cce23ec4165bc2038fb139f963",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "a61e5a0ce38ddd610d7e8a234295e3668ec683af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/brooklyn_sales_map.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5224\\849105330.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mHP\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/brooklyn_sales_map.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/brooklyn_sales_map.csv'"
     ]
    }
   ],
   "source": [
    "HP=pd.read_csv('../input/brooklyn_sales_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "4745e2f076b0d8a8c0ec6f9c1c8402b259ebfff4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5224\\2755204397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mHP\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'HP' is not defined"
     ]
    }
   ],
   "source": [
    "HP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b381179e6bd84232dbc7f27a4b2ac92607337627"
   },
   "source": [
    "We describe the data and append in it the number of NaN's in each column in the last row. We see that there are many columns with NaNs. Our next step will be to clean this data as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e612155602a8ec505a5d35789607ea9ce159ab21"
   },
   "outputs": [],
   "source": [
    "HP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f4c91af93a977870aad12a62df6d0a0c1b57ae07"
   },
   "outputs": [],
   "source": [
    "HP.describe().append(HP.isnull().sum().rename('isnull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30284dd1130e5c3b6cda698f14dba8231b6401cf"
   },
   "outputs": [],
   "source": [
    "HP.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e32211c9574223aae5786c8df828cfa00ba367a9"
   },
   "source": [
    "A big simplification can be done by removing duplicate columns. Also there are some coumns refering various map ID or file numbers which supposedly have no impact on any of the predictions made. We can easily remove 59 columns on various grounds.\n",
    "(0) 'Unnamed: 0' It's the ID number associated with each sale.\n",
    "\n",
    "(1) We remove both 'borough' & 'Borough' since we are working on Brooklyn borough only.\n",
    "\n",
    "(2) 'Unnamed: 0 no description\n",
    "\n",
    "(3) 'apartment_number' more than 90% missing data\n",
    "\n",
    "(4) 'Ext' EXTENSION CODE\n",
    "\n",
    "(5) 'Landmark' we delete this because we also have address and neighbourhood to specify the importance of the location\n",
    "\n",
    "(6) 'AreaSource' A code indicating the source file that was used to determine the tax lot's TOTAL BUILDING FLOOR AREA (BldgArea)\n",
    "\n",
    "(7) 'UnitsRes' The sum of residential units in all buildings on the tax lot. Same as 'residential_units'\n",
    "\n",
    "(8) 'UnitsTotal' The sum of residential and non-residential (offices, retail stores, etc.) units in all buildings on the tax lot. Same as 'total_units'.\n",
    "\n",
    "(9) 'LotArea' Total area of the tax lot, expressed in square feet rounded to the nearest integer. This is same as 'lot'\n",
    "\n",
    "(10) 'BldgArea' The total gross area in square feet. Same as 'gross_sqft'\n",
    "\n",
    "(11) 'BldgClass' Same as 'building_class'\n",
    "\n",
    "(12) 'Easements', 'easement' we delete both. The number of easements on the tax lot. As this is not important for most of the sales.\n",
    "\n",
    "(13)'OwnerType' A code indicating type of ownership for the tax lot\n",
    "\n",
    "(14) 'building_class_category' same as 'building_class'\n",
    "\n",
    "(15) 'sale_date' date of sale but we keep only the year in 'year_of_sale'\n",
    "\n",
    "(16) 'CT2010' The 2010 census tract that the tax lot is located in. Not considered to be important.\n",
    "\n",
    "(17)'CB2010' The 2010 census block that the tax lot is located in. Not considered to be important.\n",
    "\n",
    "(18) 'ZipCode' same as 'zip_code'\n",
    "\n",
    "(19) 'ZoneDist1' The zoning district classification of the tax lot, ZONING DISTRICT 1 represents the zoning district classification occupying the greatest percentage of the tax lot’s area.\n",
    "\n",
    "     'ZoneDist2' If the tax lot is divided by zoning boundary lines,Zoning, ZONING DISTRICT 2 represents the zoning classification occupying the second greatest percentage of the tax lot's area.\n",
    "     \n",
    "     'ZoneDist3'If the tax lot is divided by zoning boundary lines, ZONING, ZONING DISTRICT 3 represents the zoning classification occupying the third greatest percentage of the tax lot's area. \n",
    "     \n",
    "     'ZoneDist4' If the tax lot is divided by zoning boundary lines, Zoning, ZONING DISTRICT 4 represents the zoning classification occupying the fourth greatest percentage of the tax lot's area.\n",
    "     \n",
    "(20) 'Overlay1' The commercial overlay assigned to the tax lot. \n",
    "     'Overlay2' A commercial overlay associated with the tax lot.\n",
    "     \n",
    "(21) 'SPDist1' The special purpose district assigned to the tax lot.SPECIAL PURPOSE DISTRICT 1 represents the special purpose district occupying the greatest percentage of the lot area.\n",
    "\n",
    "     'SPDist2' SPECIAL PURPOSE DISTRICT 2 represents the special purpose district occupying the second greatest percentage of the lot area.\n",
    "     \n",
    "     'SPDist3' SPECIAL PURPOSE DISTRICT 3 represents the special purpose district occupying the smallest percentage of the lot area.\n",
    "     \n",
    "(22) 'LtdHeight' Limited height districts are coded using the three to five character district symbols\n",
    "\n",
    "(23) 'YearBuilt' same as 'year_built' \n",
    "\n",
    "(24) 'BoroCode' same as 'Borough'\n",
    "\n",
    "(25)  'BBL' A concatenation of the borough code, tax block and tax lot. \n",
    "\n",
    "(26) 'Tract2010' The 2010 census tract that the tax lot is located in.\n",
    "\n",
    "(27) 'ZoneMap' The Department of City Planning Zoning Map Number associated with the tax lot’s X and Y Coordinates.\n",
    "\n",
    "(28) 'ZMCode' A code (Y) identifies a border Tax Lot, i.e., a Tax Lot on the border of two or more Zoning Maps.\n",
    "\n",
    "(29) 'Sanborn' The Sanborn Map Company map number associated with the tax block and lot.\n",
    "\n",
    "(30) 'TaxMap' The Department of Finance paper tax map Volume Number associated with the tax block and lot.\n",
    "\n",
    "(31) 'EDesigNum' The E-Designation number assigned to the tax lot.\n",
    "\n",
    "(32) 'PLUTOMapID' A code indicating whether the tax lot is in the PLUTO file and/or the modified DTM and/or the modified DTM Clipped to the Shoreline File.\n",
    "\n",
    "(33) 'FIRM07_FLA' A one character field. Code of 1 means that some portion of the tax lot falls within the 1% annual chance floodplain as determined by FEMA’s 2007 Flood Insurance Rate Map.\n",
    "\n",
    "(34) 'PFIRM15_FL' A one character field. Code of 1 means that some portion of the tax lot falls within the 1% annual chance floodplain as determined by FEMA’s 2015 Preliminary Flood Insurance Rate Map.\n",
    "\n",
    "(35) 'Version' The Version Number related to the release of PLUTO.\n",
    "\n",
    "(36) 'MAPPLUTO_F' No description found.\n",
    "\n",
    "(37) 'APPBBL' The originating Borough, Tax Block and Tax Lot from the apportionment prior to the merge, split or property’s conversion to a condominium. The Apportionment BBL is only available for mergers, splits and conversions since 1984. \n",
    "\n",
    "(38) 'APPDate' The date of the Apportionment.\n",
    "\n",
    "(39) 'SHAPE_Leng', 'SHAPE_Area no description of both so we drop\n",
    "\n",
    "(40) 'CD' The community district (CD) or joint interest area (JIA) that the tax lot is located in, or partially located in.\n",
    "\n",
    "(41) 'SchoolDist' The community school district that the tax lot is located in. \n",
    "\n",
    "(42) 'Council'  The city council district that the tax lot is located in.\n",
    "\n",
    "(43) 'PolicePrct' The police precinct the tax lot is located in. This field contains a three digit police precinct number.\n",
    "\n",
    "(44) 'HealthCent' The health center district that the tax lot is located in.\n",
    "\n",
    "(45) 'SanitBoro' The Boro of the Sanitation District that services the tax lot. \n",
    "\n",
    "(46) 'SanitDistr' The Sanitation District that services the tax lot.\n",
    "\n",
    "(47) 'FireComp' The fire company that services the tax lot.\n",
    "\n",
    "(48) 'SanitSub' The Subsection of the Sanitation District that services the tax lot.\n",
    "\n",
    "(49) 'CondoNo' The condominium number assigned to the complex.\n",
    "\n",
    "(50) 'Address' same as 'address'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37e3f072748534f68b73241ed52598fbb442eb51",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Unnamed: 0', 'borough', 'Borough','apartment_number', 'Ext', 'Landmark','AreaSource', 'UnitsRes', 'UnitsTotal', 'LotArea', 'BldgArea','BldgClass','Easements', 'easement', 'OwnerType', 'building_class_category','sale_date', 'CT2010', 'CB2010', 'ZipCode', 'ZoneDist1', 'ZoneDist2', 'ZoneDist3', 'ZoneDist4', 'Overlay1', 'Overlay2', 'SPDist1', 'SPDist2', 'SPDist3', 'LtdHeight', 'YearBuilt', 'BoroCode', 'BBL', 'Tract2010', 'ZoneMap', 'ZMCode', 'Sanborn', 'TaxMap', 'EDesigNum', 'PLUTOMapID', 'FIRM07_FLA', 'PFIRM15_FL', 'Version', 'MAPPLUTO_F', 'APPBBL', 'APPDate', 'SHAPE_Leng', 'SHAPE_Area','CD', 'SchoolDist', 'Council', 'PolicePrct', 'HealthCent', 'SanitBoro', 'SanitDistr','FireComp','SanitSub', 'CondoNo','Address']\n",
    "HP.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cb783ca93dfb34e7deda5706c7ef08ae1567969"
   },
   "outputs": [],
   "source": [
    "HP.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d714571a84c3d0abb6d0a1d3b58627d83c42c739"
   },
   "source": [
    "We only consider rows for which the sale price is non zero. As zero sale price is associated with a transfer of property so we safely delete those. We have replaced zeros/NaNs in many variables with median/mode. Some other categorical variables were filled by 0 like 'OwnerName', 'IrrLotCode', 'SplitZone'. For 'XCoord' and 'YCoord' we have replaced the NaN and 0s by Mode(0). Same is for variables 'YearAlter1' and 'YearAlter2'. Mostly for continuous variables we have replaced the missing values by median/mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a13b0c267cce59d85814c8d291cf5265edc19f38",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP=HP[HP['sale_price']!=0]\n",
    "HP['gross_sqft']=HP['gross_sqft'].replace(0.0,HP['gross_sqft'].median())\n",
    "HP['land_sqft']=HP['land_sqft'].replace(0.0,HP['land_sqft'].median())\n",
    "HP['NumBldgs']= HP['NumBldgs'].fillna(HP['NumBldgs'].median())\n",
    "HP['NumFloors']= HP['NumFloors'].fillna(HP['NumFloors'].median())\n",
    "HP['ProxCode']= HP['ProxCode'].fillna(HP['ProxCode'].mode()[0])\n",
    "HP['LotType']= HP['LotType'].fillna(HP['LotType'].mode()[0])\n",
    "HP['BsmtCode']= HP['BsmtCode'].fillna(HP['BsmtCode'].mode()[0])\n",
    "HP['LandUse']= HP['LandUse'].fillna(HP['LandUse'].mode()[0])\n",
    "HP['AssessLand']= HP['AssessLand'].fillna(HP['AssessLand'].median())\n",
    "HP['AssessTot']= HP['AssessTot'].fillna(HP['AssessTot'].median())\n",
    "HP['ExemptLand']= HP['ExemptLand'].fillna(HP['ExemptLand'].median())\n",
    "HP['ExemptTot']= HP['ExemptTot'].fillna(HP['ExemptTot'].median())\n",
    "HP['BuiltFAR']= HP['BuiltFAR'].fillna(HP['BuiltFAR'].median())\n",
    "HP['ResidFAR']= HP['ResidFAR'].fillna(HP['ResidFAR'].median())\n",
    "HP['CommFAR']= HP['CommFAR'].fillna(HP['CommFAR'].median())\n",
    "HP['FacilFAR']= HP['FacilFAR'].fillna(HP['FacilFAR'].mean())\n",
    "HP['OwnerName']= HP['OwnerName'].fillna(value=0)\n",
    "HP['IrrLotCode']= HP['IrrLotCode'].fillna(value=0)\n",
    "HP['SplitZone']= HP['SplitZone'].fillna(value=0)\n",
    "\n",
    "HP['XCoord']= HP['XCoord'].fillna(HP['XCoord'].mode()[0])\n",
    "HP['YCoord']= HP['YCoord'].fillna(HP['YCoord'].mode()[0])\n",
    "HP['XCoord']= HP['XCoord'].replace(0.0,HP['XCoord'].mode()[0] )\n",
    "HP['YCoord']= HP['YCoord'].replace(0.0,HP['YCoord'].mode()[0] )\n",
    "\n",
    "HP['ComArea']= HP['ComArea'].fillna(HP['ComArea'].median())\n",
    "HP['ResArea']= HP['ResArea'].fillna(HP['ResArea'].median())\n",
    "HP['OfficeArea']= HP['OfficeArea'].fillna(HP['OfficeArea'].median())\n",
    "HP['RetailArea']= HP['RetailArea'].fillna(HP['RetailArea'].median())\n",
    "HP['GarageArea']= HP['GarageArea'].fillna(HP['GarageArea'].median())\n",
    "HP['OtherArea']= HP['OtherArea'].fillna(HP['OtherArea'].median())\n",
    "HP['StrgeArea']= HP['StrgeArea'].fillna(HP['StrgeArea'].median())\n",
    "HP['FactryArea']= HP['FactryArea'].fillna(HP['FactryArea'].median())\n",
    "HP['LotFront']= HP['LotFront'].fillna(HP['LotFront'].median())\n",
    "HP['LotDepth']= HP['LotDepth'].fillna(HP['LotDepth'].median())\n",
    "HP['BldgFront']= HP['BldgFront'].fillna(HP['BldgFront'].median())\n",
    "HP['BldgDepth']= HP['BldgDepth'].fillna(HP['BldgDepth'].median())\n",
    "HP['HealthArea']= HP['HealthArea'].fillna(HP['HealthArea'].median())\n",
    "HP['YearAlter1']= HP['YearAlter1'].fillna(HP['YearAlter1'].mode()[0])\n",
    "HP['YearAlter2']= HP['YearAlter2'].fillna(HP['YearAlter2'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98300dbc6e40a43ad180f2e7dccce1f1f2f77141"
   },
   "source": [
    "The categorical variables are categorized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8ce93b2fa8b54e70f0b351975e72ce061d089553"
   },
   "outputs": [],
   "source": [
    "HP['HistDist'].fillna(0.0, inplace=True)\n",
    "HP['HistDist']=HP['HistDist'].astype('category')\n",
    "HP['HistDist']=HP['HistDist'].cat.codes\n",
    "HP['HistDist'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c661bf7c332e737c5567a5f2a282654cbeb7285c",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP['neighborhood']=HP['neighborhood'].astype('category')\n",
    "HP['neighborhood']=HP['neighborhood'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5533d079548466b8ef655a24ba9be6e2429e8500"
   },
   "source": [
    "We split the 'address' into number and 'street name'. Convert the 'street name' to categorical variable and delete the number column along with 'address'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "70b4938f1e74b935c18f41713a5545caa3686daa",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP[['number','street name']] = HP['address'].str.split(n=1, expand=True)\n",
    "del HP['address']\n",
    "del HP['number']\n",
    "HP['street name']=HP['street name'].astype('category')\n",
    "HP['street name']=HP['street name'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8e2d34f9897dadc6f4d672f43bdc6c791eee78a9"
   },
   "source": [
    "We see the missing values in 'tax_class' and replace them by the corresponding values from 'tax_class_at_sale'. The convert 'tax_class' to categorical. Same is done for the missing values in 'building_class' by replacing the missing values from 'building_class_at_sale'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5f463cb7d2feb5d5293ef7cc51b8aed49046ed67"
   },
   "outputs": [],
   "source": [
    "print(HP['tax_class'].unique())\n",
    "print(HP['tax_class'].isnull().sum())\n",
    "print(HP['tax_class_at_sale'].unique())\n",
    "print(HP['tax_class_at_sale'].isnull().sum())\n",
    "HP['tax_class'] = HP['tax_class'].map({'1B': 5, '2A': 6, '2B':7, '1A':8, '2C':9, '3':3,'4':4,'2':2,'1':1})\n",
    "HP['tax_class'].fillna(HP['tax_class_at_sale'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "52ab6d4001d82a4b3458665fb1aa53ab03dbfd55",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP['building_class'].fillna(HP['building_class_at_sale'], inplace=True)\n",
    "HP['building_class']=HP['building_class'].astype('category')\n",
    "HP['building_class_at_sale']=HP['building_class_at_sale'].astype('category')\n",
    "\n",
    "cat_columns = HP.select_dtypes(['category']).columns\n",
    "cat_columns\n",
    "HP[cat_columns] = HP[cat_columns].apply(lambda x: x.cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cb20d6821e66e0a3a5b5ac11cdc74885fadcc94"
   },
   "source": [
    "We categorize several other variables 'OwnerName', 'IrrLotCode', 'SplitZone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e37dd6fc865ffefd035178a453079f072b324db4"
   },
   "outputs": [],
   "source": [
    "print(HP['OwnerName'].unique())\n",
    "HP['OwnerName']= HP['OwnerName'].fillna(value=0)\n",
    "print(HP['OwnerName'].isnull().sum())\n",
    "\n",
    "HP['OwnerName']=HP['OwnerName'].astype('category')\n",
    "HP['OwnerName']=HP['OwnerName'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cdc817467aac6c3008f5ff7a284eeaf10906b04",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP['IrrLotCode'].unique()\n",
    "HP['IrrLotCode']= HP['IrrLotCode'].fillna(value=0)\n",
    "HP['IrrLotCode']= HP['IrrLotCode'].astype('category')\n",
    "HP['IrrLotCode']= HP['IrrLotCode'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2848488a90c7ad549bea2a9fa8e4f1eeec465518",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "HP['SplitZone'].unique()\n",
    "HP['SplitZone']= HP['SplitZone'].fillna(value=0)\n",
    "HP['SplitZone']= HP['SplitZone'].astype('category')\n",
    "HP['SplitZone']= HP['SplitZone'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fe45856dbede752734b5c0f4296e57899d859121"
   },
   "source": [
    "We have thus removed all the null values from the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "862337b208e35cd2699b7608929850a6fc1b5969"
   },
   "outputs": [],
   "source": [
    "HP.describe().append(HP.isnull().sum().rename('isnull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "84d937d58ba9eda13296cf46236b07aa1c247eee"
   },
   "outputs": [],
   "source": [
    "HP.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a5d4e5676c3ccb673939e852c46b05e28b386c0b"
   },
   "outputs": [],
   "source": [
    "HP.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b9521358473bab23a260051ac0d21ca9530abf3c"
   },
   "source": [
    "We locate and remove all the outliers in each column. Which makes the data more streamlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cd38b6bd092496a1c027003b27f50de2d8a2744"
   },
   "outputs": [],
   "source": [
    "HP.boxplot(column='gross_sqft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b86ab6072114cfee2a14ad240a0a8fe6a4a0886",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "HP=HP[(np.abs(stats.zscore(HP)) < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90bf2fef7d7a55f3e84c9a81d642351942ba29c6"
   },
   "outputs": [],
   "source": [
    "HP.boxplot(column='gross_sqft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01c073e8f749321554b5adce8d750620fb011241",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import RFE\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "64b8f44b5c4d29305882315d700c096a31f0b323"
   },
   "source": [
    "Next we remove variables those are highly correlated by evaluating their Variable Inflation Factor. We remove one variable each step and again calculate the VIF and then remove the next. We remove all variables with VIF>5 by this way to minimize correlation among the variables. We show a few and the rest are done in a similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "830d3ed2ef55e4a516e59fe88b389e364b68949c",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = HP.drop('sale_price',axis=1)\n",
    "y = HP['sale_price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2218397161a1c5cf872b326999847357fd541e3"
   },
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f1042c42a11462d6449321b15e0fb8764a27c2d9",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "del HP['zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb3af11bce33d68ce446392325b805b6f5c0c0a4",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X = HP.drop('sale_price',axis=1)\n",
    "y = HP['sale_price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94794a05f50a36330e4a330fb50dde158e8a0366"
   },
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif[\"features\"] = X.columns\n",
    "vif.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6c46b69a1494722b9d8e451aa8a956d90f15dea",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "del HP['total_units']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22da47ff5cf73cd01654a06cd9e91c60524e6d6a"
   },
   "source": [
    "It should continue this way resulting in the deletion of the following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e30b11ddb71afe354967ff6f499951f9b4d5f44e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "columns = ['YearAlter2', 'XCoord', 'year_of_sale',  'YCoord', 'ComArea','LotDepth', 'LotType', 'NumFloors', 'LotFront', 'tax_class_at_sale', 'building_class', 'ResArea', 'SplitZone', 'BldgDepth', 'ResidFAR', 'LandUse', 'HealthArea', 'gross_sqft', 'BldgFront', 'year_built', 'IrrLotCode', 'AssessTot', 'land_sqft', 'FacilFAR', 'building_class_at_sale', 'NumBldgs']\n",
    "HP.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2106d44ddcfefb171db58bc25168b4c3da52d41"
   },
   "outputs": [],
   "source": [
    "HP.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ebd574e568c70fb797073d5e7a0cd7c53c696090"
   },
   "outputs": [],
   "source": [
    "HP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ce1633bd5b3b8a9be7c12438208376cd7cbbd65"
   },
   "outputs": [],
   "source": [
    "HP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "29e9278cfc8f6e01587de3f1e113eb0f021a7805"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(HP,y_vars=['sale_price'], x_vars=['AssessLand', 'HistDist', 'OtherArea', 'StrgeArea', 'YearAlter1', 'BuiltFAR'],palette='Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57d7acaa3301a0fd89e423d9e1864a4fd588a098"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(HP,y_vars=['sale_price'], x_vars=['commercial_units', 'lot', 'neighborhood', 'residential_units', 'tax_class', 'BsmtCode'],palette='Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bd4a4f72bad894dc8f8cf754d4bb7dd52e53c559"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(HP,y_vars=['sale_price'], x_vars=['GarageArea', 'OwnerName', 'ExemptTot', 'FactryArea', 'block'],palette='Dark2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "703d492fccb0b9c1fbfe180d92e5a05c772b3d8d"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(HP,y_vars=['sale_price'], x_vars=['RetailArea', 'CommFAR', 'OfficeArea', 'ProxCode', 'ExemptLand', 'street name'],palette='Dark2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b083b047af0375f3058560ebf7f5e0080fa18449"
   },
   "source": [
    "We delete few more variables which shows weak dependence on the sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd9039610b10cb86898d5025f35d76a4d107c671",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "701e838a33ee96af5e1ba7aff750241ef276b67a"
   },
   "outputs": [],
   "source": [
    "X = HP.drop('sale_price',axis=1)\n",
    "y = HP['sale_price']\n",
    "\n",
    "Xtrn, Xtest, Ytrn, Ytest = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "models = [LinearRegression(), linear_model.Lasso(alpha=0.1), Ridge(alpha=100.0), RandomForestRegressor(n_estimators=100, max_features='sqrt'), KNeighborsRegressor(n_neighbors=6),DecisionTreeRegressor(max_depth=4), ensemble.GradientBoostingRegressor()]\n",
    "\n",
    "TestModels = pd.DataFrame()\n",
    "tmp = {}\n",
    " \n",
    "for model in models:\n",
    "    print(model)\n",
    "    m = str(model)\n",
    "    tmp['Model'] = m[:m.index('(')]\n",
    "    model.fit(Xtrn, Ytrn)\n",
    "    tmp['R2_Price'] = r2_score(Ytest, model.predict(Xtest))\n",
    "    print('score on training',model.score(Xtrn, Ytrn))\n",
    "    print('r2 score',r2_score(Ytest, model.predict(Xtest)))\n",
    "    TestModels = TestModels.append([tmp])\n",
    "TestModels.set_index('Model', inplace=True)\n",
    " \n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.R2_Price.plot(ax=axes, kind='bar', title='R2_Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "519ebe4d2882dc63a6af57efe4becbcb84b52b1d"
   },
   "source": [
    "We see that GradientBoostingRegressor is the best model for us as scores on both training and testing sets are close, indicating no overfitting. So we choose it and apply grid search CV to find out best set of parameters. We dont run this step as it is really time consuming.  We state the final result and use it in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "938799aaa12b1817054c553d40f2ab8c0dd10a82",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HP.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bccfcf6fea765b4318c6325f964b783892704a38"
   },
   "outputs": [],
   "source": [
    "HP_list=list(HP.columns.values)\n",
    "HP_list1=list(HP.columns.values)\n",
    "\n",
    "names=HP_list1\n",
    "feature_cols =['neighborhood', 'tax_class', 'block', 'lot', 'residential_units', 'commercial_units', 'OwnerName', 'OfficeArea',\n",
    "       'RetailArea', 'GarageArea', 'StrgeArea', 'FactryArea', 'OtherArea', 'ProxCode', 'BsmtCode', 'AssessLand', 'ExemptLand', 'ExemptTot',\n",
    "       'YearAlter1', 'HistDist', 'BuiltFAR', 'CommFAR', 'street name']\n",
    "target=['sale_price']\n",
    "X=HP[feature_cols].dropna()\n",
    "y=np.array(HP[target].dropna()).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "#use linear regression as the model\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "#rank all features, i.e continue the elimination until the last one\n",
    "rfe = RFE(model, n_features_to_select=10, step=1)\n",
    "rfe.fit(X,y)\n",
    "print('Features sorted by their rank:')\n",
    "print(sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), names)))\n",
    "\n",
    "# Plot feature importance\n",
    "feature_importance = model.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e6d8131d81c307c1d52b86af35fdcf22a43ebb72"
   },
   "source": [
    "We drop few more columns based on the above analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "50e18dc2186efce972f3a89332d43e0337b847fb",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "columns = ['OfficeArea', 'GarageArea', 'StrgeArea', 'FactryArea', 'OtherArea', 'BsmtCode', 'CommFAR']\n",
    "HP.drop(columns, inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "742a4942e1f65816a2bd461f741e56b480a882cc"
   },
   "outputs": [],
   "source": [
    "HP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7092131c8cb3d401ab7fd06486f5ebc3efc49777"
   },
   "source": [
    "We do grid search to find out the best parameters for Gradient Boosting Regressor. Since there is a trade off between the parameters 'learning_rate' and 'n_estimators' we try to find out the best values for them. Other parameters are accepted at their default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3530175b6c171a667974a5c7e6096e50a0d177e6"
   },
   "outputs": [],
   "source": [
    "X = HP.drop('sale_price',axis=1)\n",
    "y = HP['sale_price']\n",
    "Xtrn, Xtest, Ytrn, Ytest = train_test_split(X,y,test_size=0.3, random_state=42)\n",
    "\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "parameters = {'learning_rate':[0.1,0.2,0.3],'n_estimators':[50,100,150]}\n",
    "grid_obj = GridSearchCV(model, parameters,refit=True,cv=3,verbose=10)\n",
    "grid_obj = grid_obj.fit(Xtrn, Ytrn)\n",
    "print(grid_obj.fit(Xtrn, Ytrn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a378fd43c49b4006db8593da0a7b374c4a1c4170"
   },
   "outputs": [],
   "source": [
    "grid_obj.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4683714d277d2dfd7dcbb34b7f3a82c60c5d028"
   },
   "outputs": [],
   "source": [
    "params = grid_obj.best_params_\n",
    "model = ensemble.GradientBoostingRegressor(**params)\n",
    "model.fit(Xtrn, Ytrn.values.ravel())\n",
    "test_score = np.zeros((params['n_estimators']),dtype=np.float64)\n",
    "\n",
    "# Predict\n",
    "Ypred = model.predict(Xtest)\n",
    "model_mse = mean_squared_error(Ypred, Ytest)\n",
    "model_rmse = np.sqrt(model_mse)\n",
    "print('Gradient Boosting RMSE: %.4f' % model_rmse)\n",
    "print('score on training',model.score(Xtrn, Ytrn))\n",
    "print('r2 score',r2_score(Ytest, model.predict(Xtest)))\n",
    "df = pd.DataFrame({'Actual': Ytest, 'Predicted': Ypred, 'Difference': Ypred-Ytest})\n",
    "print(df.head())\n",
    "\n",
    "for i,Ypred in enumerate(model.staged_predict(Xtest)):\n",
    "    test_score[i]=model.loss_(Ytest.values.ravel(),Ypred)\n",
    "\n",
    "# Plot training deviance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, model.train_score_, 'b-',label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-', label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c97c59646ab0c7dfd3397e19fd04ba835601aa14"
   },
   "outputs": [],
   "source": [
    "colors=('red','blue')\n",
    "plt.scatter(Ypred,Ytest,c=colors)\n",
    "plt.xlabel('sale_price_pred')\n",
    "plt.ylabel('sale_price_test')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94419c0f937146044864c3dfb55b174a2405f39c"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a718447a73c26d88b9b05364eb8beaa4e0f4802"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e68bc8e3f50bf8b81e9f4eb6997538dba7484a6c"
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
